{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7bad14d",
   "metadata": {},
   "source": [
    "\n",
    "# ðŸ§  Retrieval-Augmented Generation (RAG) Q&A Chatbot\n",
    "\n",
    "This notebook implements a lightweight Retrieval-Augmented Generation (RAG) pipeline using:\n",
    "- **FAISS** for document retrieval\n",
    "- **MiniLM** embeddings from Hugging Face\n",
    "- **Hugging Face Transformers** (`distilbert-base-uncased`) as a lightweight generative model\n",
    "\n",
    "The chatbot answers questions using context retrieved from a custom knowledge base created from the provided **Training Dataset.csv**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc1eb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install faiss-cpu sentence-transformers transformers datasets --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ee8022",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718bdac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the Training Dataset\n",
    "df = pd.read_csv(\"Training Dataset.csv\")\n",
    "\n",
    "# Show basic info\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30843b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert rows to a text corpus (e.g., concatenating all columns)\n",
    "corpus = df.apply(lambda row: \" | \".join([str(cell) for cell in row]), axis=1).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0823e172",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(corpus, show_progress_bar=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb9f17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(np.array(embeddings))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97a7681",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-uncased\", tokenizer=\"distilbert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0d1259",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rag_qa(user_query, top_k=5):\n",
    "    query_embedding = model.encode([user_query])\n",
    "    D, I = index.search(np.array(query_embedding), top_k)\n",
    "    retrieved_docs = [corpus[i] for i in I[0]]\n",
    "    context = \" \".join(retrieved_docs)\n",
    "    answer = qa_pipeline(question=user_query, context=context)\n",
    "    return answer['answer']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc72861",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage\n",
    "user_question = \"What is the age of the customer with the highest score?\"\n",
    "answer = rag_qa(user_question)\n",
    "print(\"Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39a3a74",
   "metadata": {},
   "source": [
    "\n",
    "### ðŸ“Œ Notes:\n",
    "- This notebook uses `distilbert-base-uncased` as a small open model. You can swap it with a larger model if you have more compute or use OpenAI/Gemini APIs by modifying the `rag_qa` function.\n",
    "- The knowledge base is created from the Training Dataset CSV, converted into a text corpus row-wise.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
